{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "17FgqoDpfHaBVfjr4Pi5mjCHeBJ9lU93v",
      "authorship_tag": "ABX9TyPW8zaMunyF82oy8L+wpR4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jgallear/CSSP_brazil_23_24/blob/main/cross_validate_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfOX6qpuLdOx",
        "outputId": "0ebb5e11-aa7a-4dd7-8b7b-0f4ccf9f282a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GtspX8aaAQe",
        "outputId": "bb44246b-c61c-4644-ff8a-cbaedd76771c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (24.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PiwAdyQgIt9",
        "outputId": "e3f85914-af2f-47b2-ec60-2210024d5cf5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "#from rasterio import CRS\n",
        "import functools\n",
        "import pickle\n",
        "#import cartopy.feature as cf\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "import itertools\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from keras.optimizers import RMSprop, Adam, SGD\n",
        "#from sklearn.model_selection import RandomizedSearchCV\n",
        "#from scikeras.wrappers import KerasRegressor\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout"
      ],
      "metadata": {
        "id": "1H-04PveZ9ea"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to determine split, take random set of 5 years, create evaluation period, then split training in half, and test number of estimators on each half,\n",
        "# this will mean model is trained number of folds * number of estimators times. (2*6) times"
      ],
      "metadata": {
        "id": "8_sx8KVdbdhR"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do cv split based on year, split years into 2 folds and evaluate a range of n_estimators from 2, 10, 50, 100, 200, 500"
      ],
      "metadata": {
        "id": "gYaNa_vJaCvj"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load df\n",
        "df = pd.read_csv('/content/drive/MyDrive/VHI_spei_rzsm_dataset.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "158Bb6Y-am6b",
        "outputId": "8035fc45-1e99-4fb8-973d-3c30a82e9ad7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           lon     lat      vhi-1   precip-1      rzsm-1  spei1-1m  spei2-1m  \\\n",
            "0      -53.375 -32.125  70.414172  259.20056   98.021910  2.192168  1.313241   \n",
            "1      -53.375 -32.125  79.323750  146.58607   98.162980  1.228278  2.453098   \n",
            "2      -53.375 -32.125  77.111895  184.82845   97.594270  1.472914  1.588110   \n",
            "3      -53.375 -32.125  64.150561  108.86273   97.605330  1.680747  1.751371   \n",
            "4      -53.375 -32.125  54.270359  145.90760  100.000000  1.641170  1.900034   \n",
            "...        ...     ...        ...        ...         ...       ...       ...   \n",
            "577682 -47.625  -0.875  41.102345  170.38158   66.929490  0.324701  0.572972   \n",
            "577683 -47.625  -0.875  49.922513  294.35620   80.138200  1.130223  0.881931   \n",
            "577684 -47.625  -0.875  52.101102  125.48111   88.888840  2.664571  2.250521   \n",
            "577685 -47.625  -0.875  46.753572   24.13338   77.834076  2.523025  3.002636   \n",
            "577686 -47.625  -0.875  48.627606   71.58264   69.860950  1.286047  2.275886   \n",
            "\n",
            "        spei3-1m   spi01-1   spi02-1   spi03-1     pev-1   longrad-1  \\\n",
            "0       2.266991  1.718010  1.131672  1.956504 -0.004538  387.621424   \n",
            "1       1.678403  1.015022  1.645683  1.368535 -0.003293  376.919222   \n",
            "2       2.096257  0.956124  1.150522  1.621207 -0.002408  345.837482   \n",
            "3       1.845372  0.412008  0.789140  1.030642 -0.001716  334.681940   \n",
            "4       1.926722  0.966855  0.774759  0.982040 -0.001001  338.842544   \n",
            "...          ...       ...       ...       ...       ...         ...   \n",
            "577682  1.015577  1.146655  0.969593  0.674176 -0.004356  412.320024   \n",
            "577683  0.859094  1.573595  1.592416  1.312561 -0.004909  410.370278   \n",
            "577684  1.929124  2.537942  2.341496  2.211642 -0.005699  410.550830   \n",
            "577685  2.507460  1.575954  2.610072  2.547964 -0.005919  415.099818   \n",
            "577686  2.791671  1.595141  2.002994  2.608675 -0.005324  417.379510   \n",
            "\n",
            "             t2m-1  month  year        VHI  \n",
            "0       296.530026      3  2003  79.323750  \n",
            "1       294.303754      4  2003  77.111895  \n",
            "2       290.164102      5  2003  64.150561  \n",
            "3       288.467444      6  2003  54.270359  \n",
            "4       286.129896      7  2003  59.200781  \n",
            "...            ...    ...   ...        ...  \n",
            "577682  299.706968      7  2021  49.922513  \n",
            "577683  300.173380      8  2021  52.101102  \n",
            "577684  300.253404      9  2021  46.753572  \n",
            "577685  300.919370     10  2021  48.627606  \n",
            "577686  300.832808     11  2021  47.450658  \n",
            "\n",
            "[577687 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need to loop through df separate specified years into train and test\n",
        "n_estimators_list = [2,10,25,50,100,200,500]\n",
        "# obtain random set of years for testing\n",
        "print(df['year'].unique())\n",
        "test_yrs = np.random.choice(df['year'].unique(),replace=False,size=5)\n",
        "print(test_yrs)"
      ],
      "metadata": {
        "id": "w2VrCCPReyGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb121c47-33a0-4115-a7f8-830577de2ce2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016\n",
            " 2017 2018 2019 2020 2021]\n",
            "[2015 2014 2007 2008 2009]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_yrs = [2016, 2011, 2019, 2015, 2006]"
      ],
      "metadata": {
        "id": "K1PszCA8yKQp"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove test_yrs from dataset, then split training into fold1 and fold2\n",
        "test_df = df[df['year'].isin(test_yrs)]\n",
        "###\n",
        "train_df = df[~df['year'].isin(test_yrs)]\n",
        "# split train_df in half for fold 1 and fold 2\n",
        "print(len(train_df['year'].unique()))\n",
        "print(int(len(train_df['year'].unique())/2))\n",
        "split = int(len(train_df['year'].unique())/2)\n",
        "print(train_df['year'].unique())\n",
        "f1_years = train_df['year'].unique()[:split]\n",
        "print(f1_years)\n",
        "f2_years = train_df['year'].unique()[split:]\n",
        "print(f2_years)\n",
        "## use f1 years and f2 years to split dataframe\n",
        "train_df_ri = train_df.reset_index()\n",
        "f1_df = train_df_ri[~train_df_ri['year'].isin(f1_years)]\n",
        "f2_df = train_df_ri[~train_df_ri['year'].isin(f2_years)]\n",
        "\n",
        "f1_inds = f1_df.index\n",
        "f2_inds = f2_df.index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g5g1DlLgolo",
        "outputId": "660fe907-4418-4c01-a6ff-2e28a453ce56"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n",
            "7\n",
            "[2003 2004 2005 2007 2008 2009 2010 2012 2013 2014 2017 2018 2020 2021]\n",
            "[2003 2004 2005 2007 2008 2009 2010]\n",
            "[2012 2013 2014 2017 2018 2020 2021]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max(f1_inds))\n",
        "print(max(f2_inds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF4bjzYq0Ip8",
        "outputId": "15a7c239-a3f7-410a-c72b-fc1348237693"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "425006\n",
            "424929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folds_list=[]\n",
        "folds_list.append([f1_inds,f2_inds])\n",
        "folds_list.append([f2_inds,f1_inds])"
      ],
      "metadata": {
        "id": "q4CLsW6ev34A"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(f1_inds))\n",
        "print(len(f2_inds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-q76pxhqXn1",
        "outputId": "e9062289-b2ff-4d7f-8797-59f2befdb430"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "213752\n",
            "211255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group = df.year.tolist()\n",
        "target='VHI'\n",
        "y = df[target]\n",
        "x = df.drop(columns=[target])"
      ],
      "metadata": {
        "id": "vLnFZJ0d7XyM"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gkf = GroupKFold(n_splits=2)\n",
        "for train_index, test_index in gkf.split(x,groups=group):\n",
        "  print('train: ',train_index, 'test: ', test_index)\n",
        "  xtrain = x.iloc[train_index,:]\n",
        "  ytrain = y[train_index]\n",
        "  xtest = x.iloc[test_index,:]\n",
        "  ytest = y[test_index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD2lLFLx7_1m",
        "outputId": "e433e433-824d-4427-9a13-d1bbd03c2f7a"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:  [    21     22     23 ... 577662 577663 577664] test:  [     0      1      2 ... 577684 577685 577686]\n",
            "train:  [     0      1      2 ... 577684 577685 577686] test:  [    21     22     23 ... 577662 577663 577664]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xtrain['year'].unique())\n",
        "print(xtest['year'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpOAyrAU-glz",
        "outputId": "d4d9fe51-57d1-4fee-ed61-9c6e24af58f8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2003 2004 2006 2008 2010 2014 2016 2018 2020 2021]\n",
            "[2005 2007 2009 2011 2012 2013 2015 2017 2019]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define estimator\n",
        "def create_ff_model(neurons=500, dropout_rate=0, activation='relu', init_mode='RandomNormal', learn_rate=0.001, optimizer='RMSprop'):\n",
        "    model=Sequential()\n",
        "    model.add(Dense(neurons,input_shape=(16,),kernel_initializer=init_mode, activation='linear'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation, kernel_regularizer='l2'))\n",
        "    #model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation))\n",
        "    #model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation))\n",
        "    #model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    #compile\n",
        "    model.compile(loss='mae', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XnVP08VAWjHu"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use grid search cv\n",
        "# optimize number of estimators\n",
        "    # Define hyperparameter space\n",
        "batch_size = [int(x) for x in np.linspace(start = 5, stop = 100, num = 5)]\n",
        "epochs = np.array((5,10,30))\n",
        "optimizer = ['Adam', 'RMSprop', 'Nadam']\n",
        "dropout_rate = [int(x) for x in np.linspace(start = 0.1, stop = 1, num = 5)]\n",
        "#neurons = [int(x) for x in np.linspace(start = 5, stop = 1000, num = 50)]\n",
        "activation = ['relu', 'selu', 'elu']\n",
        "learn_rate = np.array((0.0001, 0.01, 0.1))\n",
        "init_mode = ['RandomNormal', 'he_normal', 'VarianceScaling', 'TruncatedNormal']\n",
        "neurons = np.array((25, 50, 250))\n",
        "dropout_rate = np.array((0.0, 0.2, 0.4))\n",
        "# can add settings to grid then add for gridsearchcv\n",
        "param_grid = {'neurons':neurons,'epochs':epochs}\n",
        "\n",
        "model = KerasRegressor(build_fn=create_ff_model, batch_size=10, epochs =1)\n",
        "\n",
        "grid_cv = GridSearchCV(model,param_grid,cv=folds_list,scoring='r2',verbose=3)"
      ],
      "metadata": {
        "id": "SnwJotUvgzt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebdbce9-c80c-4888-d1e3-9136c1ae7266"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-97-2fc2fa1a1dbb>:17: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasRegressor(build_fn=create_ff_model, batch_size=10, epochs =1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1-phtIYEMoM",
        "outputId": "0d28f6c3-7de6-4593-a339-433005301236"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           lon     lat      vhi-1   precip-1      rzsm-1  spei1-1m  spei2-1m  \\\n",
            "0      -53.375 -32.125  70.414172  259.20056   98.021910  2.192168  1.313241   \n",
            "1      -53.375 -32.125  79.323750  146.58607   98.162980  1.228278  2.453098   \n",
            "2      -53.375 -32.125  77.111895  184.82845   97.594270  1.472914  1.588110   \n",
            "3      -53.375 -32.125  64.150561  108.86273   97.605330  1.680747  1.751371   \n",
            "4      -53.375 -32.125  54.270359  145.90760  100.000000  1.641170  1.900034   \n",
            "...        ...     ...        ...        ...         ...       ...       ...   \n",
            "577682 -47.625  -0.875  41.102345  170.38158   66.929490  0.324701  0.572972   \n",
            "577683 -47.625  -0.875  49.922513  294.35620   80.138200  1.130223  0.881931   \n",
            "577684 -47.625  -0.875  52.101102  125.48111   88.888840  2.664571  2.250521   \n",
            "577685 -47.625  -0.875  46.753572   24.13338   77.834076  2.523025  3.002636   \n",
            "577686 -47.625  -0.875  48.627606   71.58264   69.860950  1.286047  2.275886   \n",
            "\n",
            "        spei3-1m   spi01-1   spi02-1   spi03-1     pev-1   longrad-1  \\\n",
            "0       2.266991  1.718010  1.131672  1.956504 -0.004538  387.621424   \n",
            "1       1.678403  1.015022  1.645683  1.368535 -0.003293  376.919222   \n",
            "2       2.096257  0.956124  1.150522  1.621207 -0.002408  345.837482   \n",
            "3       1.845372  0.412008  0.789140  1.030642 -0.001716  334.681940   \n",
            "4       1.926722  0.966855  0.774759  0.982040 -0.001001  338.842544   \n",
            "...          ...       ...       ...       ...       ...         ...   \n",
            "577682  1.015577  1.146655  0.969593  0.674176 -0.004356  412.320024   \n",
            "577683  0.859094  1.573595  1.592416  1.312561 -0.004909  410.370278   \n",
            "577684  1.929124  2.537942  2.341496  2.211642 -0.005699  410.550830   \n",
            "577685  2.507460  1.575954  2.610072  2.547964 -0.005919  415.099818   \n",
            "577686  2.791671  1.595141  2.002994  2.608675 -0.005324  417.379510   \n",
            "\n",
            "             t2m-1  month  year        VHI  \n",
            "0       296.530026      3  2003  79.323750  \n",
            "1       294.303754      4  2003  77.111895  \n",
            "2       290.164102      5  2003  64.150561  \n",
            "3       288.467444      6  2003  54.270359  \n",
            "4       286.129896      7  2003  59.200781  \n",
            "...            ...    ...   ...        ...  \n",
            "577682  299.706968      7  2021  49.922513  \n",
            "577683  300.173380      8  2021  52.101102  \n",
            "577684  300.253404      9  2021  46.753572  \n",
            "577685  300.919370     10  2021  48.627606  \n",
            "577686  300.832808     11  2021  47.450658  \n",
            "\n",
            "[425007 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainx = train_df.drop(columns=['VHI'])\n",
        "trainy = train_df['VHI']\n",
        "# scale inputs\n",
        "scaler = StandardScaler()\n",
        "trainx_sc = scaler.fit_transform(trainx)"
      ],
      "metadata": {
        "id": "8Zv1ssyoEQmT"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainx.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjsqEctig89F",
        "outputId": "1fc6d579-108d-4635-e1d8-8aebb2eb02b4"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(425007, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trainy))\n",
        "print(len(trainx_sc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISNGaro9yUEk",
        "outputId": "6cf1ac87-90af-4a6e-c3c6-24a922182d71"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "425007\n",
            "425007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIklsJ0V0t6I",
        "outputId": "3614ff10-b3c1-47a7-f114-b0d2444b2dce"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0         79.323750\n",
            "1         77.111895\n",
            "2         64.150561\n",
            "3         54.270359\n",
            "4         59.200781\n",
            "            ...    \n",
            "577682    49.922513\n",
            "577683    52.101102\n",
            "577684    46.753572\n",
            "577685    48.627606\n",
            "577686    47.450658\n",
            "Name: VHI, Length: 425007, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_result = grid_cv.fit(trainx_sc,trainy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTEKnrfJKmX4",
        "outputId": "7fe7e646-2ac7-43cf-fc5a-044aa92fbb34"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
            "Epoch 1/5\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 7.3882\n",
            "Epoch 2/5\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.6404\n",
            "Epoch 3/5\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.6130\n",
            "Epoch 4/5\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.6005\n",
            "Epoch 5/5\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5939\n",
            "21126/21126 [==============================] - 26s 1ms/step\n",
            "[CV 1/2] END ..............epochs=5, neurons=25;, score=0.592 total time= 3.9min\n",
            "Epoch 1/5\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.9558\n",
            "Epoch 2/5\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1370\n",
            "Epoch 3/5\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1154\n",
            "Epoch 4/5\n",
            "21126/21126 [==============================] - 32s 2ms/step - loss: 6.1016\n",
            "Epoch 5/5\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.0901\n",
            "21376/21376 [==============================] - 25s 1ms/step\n",
            "[CV 2/2] END ..............epochs=5, neurons=25;, score=0.534 total time= 3.3min\n",
            "Epoch 1/5\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 7.0113\n",
            "Epoch 2/5\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5338\n",
            "Epoch 3/5\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.5072\n",
            "Epoch 4/5\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.4259\n",
            "Epoch 5/5\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.3587\n",
            "21126/21126 [==============================] - 25s 1ms/step\n",
            "[CV 1/2] END ..............epochs=5, neurons=50;, score=0.604 total time= 3.9min\n",
            "Epoch 1/5\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.6010\n",
            "Epoch 2/5\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.9693\n",
            "Epoch 3/5\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.8362\n",
            "Epoch 4/5\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.7154\n",
            "Epoch 5/5\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.6119\n",
            "21376/21376 [==============================] - 25s 1ms/step\n",
            "[CV 2/2] END ..............epochs=5, neurons=50;, score=0.559 total time= 3.3min\n",
            "Epoch 1/5\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 6.9384\n",
            "Epoch 2/5\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 6.2851\n",
            "Epoch 3/5\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 6.0684\n",
            "Epoch 4/5\n",
            "21376/21376 [==============================] - 46s 2ms/step - loss: 5.9468\n",
            "Epoch 5/5\n",
            "21376/21376 [==============================] - 48s 2ms/step - loss: 5.8567\n",
            "21126/21126 [==============================] - 28s 1ms/step\n",
            "[CV 1/2] END .............epochs=5, neurons=250;, score=0.520 total time= 4.3min\n",
            "Epoch 1/5\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 6.5006\n",
            "Epoch 2/5\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 5.9616\n",
            "Epoch 3/5\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 5.7226\n",
            "Epoch 4/5\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 5.5218\n",
            "Epoch 5/5\n",
            "21126/21126 [==============================] - 44s 2ms/step - loss: 5.3977\n",
            "21376/21376 [==============================] - 28s 1ms/step\n",
            "[CV 2/2] END .............epochs=5, neurons=250;, score=0.500 total time= 5.1min\n",
            "Epoch 1/10\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 7.3896\n",
            "Epoch 2/10\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 6.5473\n",
            "Epoch 3/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5111\n",
            "Epoch 4/10\n",
            "21376/21376 [==============================] - 33s 2ms/step - loss: 6.4976\n",
            "Epoch 5/10\n",
            "21376/21376 [==============================] - 33s 2ms/step - loss: 6.4881\n",
            "Epoch 6/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.4876\n",
            "Epoch 7/10\n",
            "21376/21376 [==============================] - 33s 2ms/step - loss: 6.4795\n",
            "Epoch 8/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.4313\n",
            "Epoch 9/10\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.3517\n",
            "Epoch 10/10\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.3175\n",
            "21126/21126 [==============================] - 26s 1ms/step\n",
            "[CV 1/2] END .............epochs=10, neurons=25;, score=0.605 total time= 6.3min\n",
            "Epoch 1/10\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.8010\n",
            "Epoch 2/10\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1429\n",
            "Epoch 3/10\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1200\n",
            "Epoch 4/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1089\n",
            "Epoch 5/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.0852\n",
            "Epoch 6/10\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.0260\n",
            "Epoch 7/10\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.0073\n",
            "Epoch 8/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.9913\n",
            "Epoch 9/10\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.9560\n",
            "Epoch 10/10\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.9078\n",
            "21376/21376 [==============================] - 25s 1ms/step\n",
            "[CV 2/2] END .............epochs=10, neurons=25;, score=0.464 total time= 6.2min\n",
            "Epoch 1/10\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 7.0134\n",
            "Epoch 2/10\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.4537\n",
            "Epoch 3/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.4218\n",
            "Epoch 4/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.4032\n",
            "Epoch 5/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.3441\n",
            "Epoch 6/10\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.2669\n",
            "Epoch 7/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.2267\n",
            "Epoch 8/10\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.1458\n",
            "Epoch 9/10\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.0167\n",
            "Epoch 10/10\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 5.9382\n",
            "21126/21126 [==============================] - 25s 1ms/step\n",
            "[CV 1/2] END .............epochs=10, neurons=50;, score=0.591 total time= 6.9min\n",
            "Epoch 1/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.6413\n",
            "Epoch 2/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.9906\n",
            "Epoch 3/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.9471\n",
            "Epoch 4/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.8779\n",
            "Epoch 5/10\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.8487\n",
            "Epoch 6/10\n",
            "21126/21126 [==============================] - 37s 2ms/step - loss: 5.8316\n",
            "Epoch 7/10\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.8127\n",
            "Epoch 8/10\n",
            "21126/21126 [==============================] - 37s 2ms/step - loss: 5.7703\n",
            "Epoch 9/10\n",
            "21126/21126 [==============================] - 36s 2ms/step - loss: 5.7291\n",
            "Epoch 10/10\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.7039\n",
            "21376/21376 [==============================] - 25s 1ms/step\n",
            "[CV 2/2] END .............epochs=10, neurons=50;, score=0.533 total time= 6.4min\n",
            "Epoch 1/10\n",
            "21376/21376 [==============================] - 46s 2ms/step - loss: 6.9578\n",
            "Epoch 2/10\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 6.3270\n",
            "Epoch 3/10\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 6.1070\n",
            "Epoch 4/10\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 5.9671\n",
            "Epoch 5/10\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.8526\n",
            "Epoch 6/10\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.7841\n",
            "Epoch 7/10\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.7245\n",
            "Epoch 8/10\n",
            "21376/21376 [==============================] - 42s 2ms/step - loss: 5.6841\n",
            "Epoch 9/10\n",
            "21376/21376 [==============================] - 42s 2ms/step - loss: 5.6563\n",
            "Epoch 10/10\n",
            "21376/21376 [==============================] - 45s 2ms/step - loss: 5.6243\n",
            "21126/21126 [==============================] - 27s 1ms/step\n",
            "[CV 1/2] END ............epochs=10, neurons=250;, score=0.535 total time= 7.8min\n",
            "Epoch 1/10\n",
            "21126/21126 [==============================] - 42s 2ms/step - loss: 6.5336\n",
            "Epoch 2/10\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 6.0251\n",
            "Epoch 3/10\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 5.7987\n",
            "Epoch 4/10\n",
            "21126/21126 [==============================] - 43s 2ms/step - loss: 5.5912\n",
            "Epoch 5/10\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 5.4636\n",
            "Epoch 6/10\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 5.3593\n",
            "Epoch 7/10\n",
            "21126/21126 [==============================] - 45s 2ms/step - loss: 5.2829\n",
            "Epoch 8/10\n",
            "21126/21126 [==============================] - 44s 2ms/step - loss: 5.2295\n",
            "Epoch 9/10\n",
            "21126/21126 [==============================] - 43s 2ms/step - loss: 5.1911\n",
            "Epoch 10/10\n",
            "21126/21126 [==============================] - 43s 2ms/step - loss: 5.1618\n",
            "21376/21376 [==============================] - 27s 1ms/step\n",
            "[CV 2/2] END ............epochs=10, neurons=250;, score=0.424 total time= 7.9min\n",
            "Epoch 1/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 7.3447\n",
            "Epoch 2/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.6254\n",
            "Epoch 3/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.6056\n",
            "Epoch 4/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.5982\n",
            "Epoch 5/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.5945\n",
            "Epoch 6/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5899\n",
            "Epoch 7/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.5864\n",
            "Epoch 8/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5823\n",
            "Epoch 9/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 6.5769\n",
            "Epoch 10/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5721\n",
            "Epoch 11/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5699\n",
            "Epoch 12/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5655\n",
            "Epoch 13/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5648\n",
            "Epoch 14/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5614\n",
            "Epoch 15/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5600\n",
            "Epoch 16/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5577\n",
            "Epoch 17/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5568\n",
            "Epoch 18/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5556\n",
            "Epoch 19/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5552\n",
            "Epoch 20/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.5551\n",
            "Epoch 21/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5550\n",
            "Epoch 22/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.5536\n",
            "Epoch 23/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5521\n",
            "Epoch 24/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5510\n",
            "Epoch 25/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5519\n",
            "Epoch 26/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5500\n",
            "Epoch 27/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5525\n",
            "Epoch 28/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 6.5502\n",
            "Epoch 29/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5507\n",
            "Epoch 30/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 6.5494\n",
            "21126/21126 [==============================] - 26s 1ms/step\n",
            "[CV 1/2] END .............epochs=30, neurons=25;, score=0.610 total time=18.1min\n",
            "Epoch 1/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.9257\n",
            "Epoch 2/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.2531\n",
            "Epoch 3/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.2329\n",
            "Epoch 4/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.2228\n",
            "Epoch 5/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.2189\n",
            "Epoch 6/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.2155\n",
            "Epoch 7/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.2121\n",
            "Epoch 8/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.2074\n",
            "Epoch 9/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.2072\n",
            "Epoch 10/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.2005\n",
            "Epoch 11/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1979\n",
            "Epoch 12/30\n",
            "21126/21126 [==============================] - 32s 2ms/step - loss: 6.1960\n",
            "Epoch 13/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1924\n",
            "Epoch 14/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1900\n",
            "Epoch 15/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1877\n",
            "Epoch 16/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1864\n",
            "Epoch 17/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1863\n",
            "Epoch 18/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1829\n",
            "Epoch 19/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1803\n",
            "Epoch 20/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1791\n",
            "Epoch 21/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1795\n",
            "Epoch 22/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1772\n",
            "Epoch 23/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1773\n",
            "Epoch 24/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1763\n",
            "Epoch 25/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1773\n",
            "Epoch 26/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1758\n",
            "Epoch 27/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 6.1756\n",
            "Epoch 28/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1752\n",
            "Epoch 29/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.1749\n",
            "Epoch 30/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 6.1754\n",
            "21376/21376 [==============================] - 26s 1ms/step\n",
            "[CV 2/2] END .............epochs=30, neurons=25;, score=0.578 total time=17.4min\n",
            "Epoch 1/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 7.0699\n",
            "Epoch 2/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 6.3434\n",
            "Epoch 3/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 6.1355\n",
            "Epoch 4/30\n",
            "21376/21376 [==============================] - 38s 2ms/step - loss: 6.0310\n",
            "Epoch 5/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.9620\n",
            "Epoch 6/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.9129\n",
            "Epoch 7/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 5.8621\n",
            "Epoch 8/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 5.8127\n",
            "Epoch 9/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.7783\n",
            "Epoch 10/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.7480\n",
            "Epoch 11/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 5.7222\n",
            "Epoch 12/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 5.7083\n",
            "Epoch 13/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.6932\n",
            "Epoch 14/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.6762\n",
            "Epoch 15/30\n",
            "21376/21376 [==============================] - 37s 2ms/step - loss: 5.6630\n",
            "Epoch 16/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.6467\n",
            "Epoch 17/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.6407\n",
            "Epoch 18/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.6355\n",
            "Epoch 19/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.6262\n",
            "Epoch 20/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 5.6148\n",
            "Epoch 21/30\n",
            "21376/21376 [==============================] - 34s 2ms/step - loss: 5.6100\n",
            "Epoch 22/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.6083\n",
            "Epoch 23/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.6046\n",
            "Epoch 24/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.5986\n",
            "Epoch 25/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.5985\n",
            "Epoch 26/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.5970\n",
            "Epoch 27/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.5896\n",
            "Epoch 28/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.5819\n",
            "Epoch 29/30\n",
            "21376/21376 [==============================] - 35s 2ms/step - loss: 5.5732\n",
            "Epoch 30/30\n",
            "21376/21376 [==============================] - 36s 2ms/step - loss: 5.5674\n",
            "21126/21126 [==============================] - 26s 1ms/step\n",
            "[CV 1/2] END .............epochs=30, neurons=50;, score=0.436 total time=18.4min\n",
            "Epoch 1/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 6.5301\n",
            "Epoch 2/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.9901\n",
            "Epoch 3/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.8853\n",
            "Epoch 4/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.7765\n",
            "Epoch 5/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.6966\n",
            "Epoch 6/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.6132\n",
            "Epoch 7/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.5389\n",
            "Epoch 8/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.4913\n",
            "Epoch 9/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.4445\n",
            "Epoch 10/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.4101\n",
            "Epoch 11/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.3890\n",
            "Epoch 12/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.3661\n",
            "Epoch 13/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.3543\n",
            "Epoch 14/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.3432\n",
            "Epoch 15/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.3334\n",
            "Epoch 16/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.3221\n",
            "Epoch 17/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.3122\n",
            "Epoch 18/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.3031\n",
            "Epoch 19/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.2975\n",
            "Epoch 20/30\n",
            "21126/21126 [==============================] - 35s 2ms/step - loss: 5.2863\n",
            "Epoch 21/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.2812\n",
            "Epoch 22/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.2716\n",
            "Epoch 23/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.2683\n",
            "Epoch 24/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.2604\n",
            "Epoch 25/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.2537\n",
            "Epoch 26/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.2418\n",
            "Epoch 27/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.2350\n",
            "Epoch 28/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.2303\n",
            "Epoch 29/30\n",
            "21126/21126 [==============================] - 34s 2ms/step - loss: 5.2273\n",
            "Epoch 30/30\n",
            "21126/21126 [==============================] - 33s 2ms/step - loss: 5.2219\n",
            "21376/21376 [==============================] - 25s 1ms/step\n",
            "[CV 2/2] END .............epochs=30, neurons=50;, score=0.278 total time=17.3min\n",
            "Epoch 1/30\n",
            "21376/21376 [==============================] - 45s 2ms/step - loss: 6.9552\n",
            "Epoch 2/30\n",
            "21376/21376 [==============================] - 42s 2ms/step - loss: 6.3590\n",
            "Epoch 3/30\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 6.0850\n",
            "Epoch 4/30\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.9371\n",
            "Epoch 5/30\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.8346\n",
            "Epoch 6/30\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.7674\n",
            "Epoch 7/30\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 5.7093\n",
            "Epoch 8/30\n",
            "21376/21376 [==============================] - 47s 2ms/step - loss: 5.6568\n",
            "Epoch 9/30\n",
            "21376/21376 [==============================] - 48s 2ms/step - loss: 5.6174\n",
            "Epoch 10/30\n",
            "21376/21376 [==============================] - 45s 2ms/step - loss: 5.5925\n",
            "Epoch 11/30\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 5.5740\n",
            "Epoch 12/30\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 5.5535\n",
            "Epoch 13/30\n",
            "21376/21376 [==============================] - 45s 2ms/step - loss: 5.5392\n",
            "Epoch 14/30\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 5.5262\n",
            "Epoch 15/30\n",
            "21376/21376 [==============================] - 47s 2ms/step - loss: 5.5097\n",
            "Epoch 16/30\n",
            "21376/21376 [==============================] - 47s 2ms/step - loss: 5.5009\n",
            "Epoch 17/30\n",
            "21376/21376 [==============================] - 46s 2ms/step - loss: 5.4907\n",
            "Epoch 18/30\n",
            "21376/21376 [==============================] - 44s 2ms/step - loss: 5.4819\n",
            "Epoch 19/30\n",
            "21376/21376 [==============================] - 42s 2ms/step - loss: 5.4717\n",
            "Epoch 20/30\n",
            "21376/21376 [==============================] - 41s 2ms/step - loss: 5.4583\n",
            "Epoch 21/30\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.4533\n",
            "Epoch 22/30\n",
            "21376/21376 [==============================] - 45s 2ms/step - loss: 5.4457\n",
            "Epoch 23/30\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.4425\n",
            "Epoch 24/30\n",
            "21376/21376 [==============================] - 41s 2ms/step - loss: 5.4377\n",
            "Epoch 25/30\n",
            "21376/21376 [==============================] - 42s 2ms/step - loss: 5.4308\n",
            "Epoch 26/30\n",
            "21376/21376 [==============================] - 42s 2ms/step - loss: 5.4171\n",
            "Epoch 27/30\n",
            "21376/21376 [==============================] - 43s 2ms/step - loss: 5.4153\n",
            "Epoch 28/30\n",
            "21376/21376 [==============================] - 48s 2ms/step - loss: 5.4093\n",
            "Epoch 29/30\n",
            "21376/21376 [==============================] - 48s 2ms/step - loss: 5.4088\n",
            "Epoch 30/30\n",
            "21376/21376 [==============================] - 48s 2ms/step - loss: 5.4034\n",
            "21126/21126 [==============================] - 27s 1ms/step\n",
            "[CV 1/2] END ...........epochs=30, neurons=250;, score=-0.119 total time=22.7min\n",
            "Epoch 1/30\n",
            "21126/21126 [==============================] - 42s 2ms/step - loss: 6.5172\n",
            "Epoch 2/30\n",
            "21126/21126 [==============================] - 42s 2ms/step - loss: 5.9922\n",
            "Epoch 3/30\n",
            "21126/21126 [==============================] - 44s 2ms/step - loss: 5.7292\n",
            "Epoch 4/30\n",
            "21126/21126 [==============================] - 42s 2ms/step - loss: 5.5571\n",
            "Epoch 5/30\n",
            "21126/21126 [==============================] - 43s 2ms/step - loss: 5.4254\n",
            "Epoch 6/30\n",
            "21126/21126 [==============================] - 43s 2ms/step - loss: 5.3206\n",
            "Epoch 7/30\n",
            "21126/21126 [==============================] - 42s 2ms/step - loss: 5.2341\n",
            "Epoch 8/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 5.1776\n",
            "Epoch 9/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 5.1426\n",
            "Epoch 10/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 5.1112\n",
            "Epoch 11/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 5.0864\n",
            "Epoch 12/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 5.0639\n",
            "Epoch 13/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 5.0459\n",
            "Epoch 14/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 5.0281\n",
            "Epoch 15/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 5.0146\n",
            "Epoch 16/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 5.0086\n",
            "Epoch 17/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9902\n",
            "Epoch 18/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9812\n",
            "Epoch 19/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 4.9764\n",
            "Epoch 20/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9703\n",
            "Epoch 21/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9640\n",
            "Epoch 22/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9592\n",
            "Epoch 23/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9578\n",
            "Epoch 24/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9497\n",
            "Epoch 25/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9449\n",
            "Epoch 26/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9405\n",
            "Epoch 27/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 4.9312\n",
            "Epoch 28/30\n",
            "21126/21126 [==============================] - 40s 2ms/step - loss: 4.9296\n",
            "Epoch 29/30\n",
            "21126/21126 [==============================] - 41s 2ms/step - loss: 4.9254\n",
            "Epoch 30/30\n",
            "21126/21126 [==============================] - 42s 2ms/step - loss: 4.9248\n",
            "21376/21376 [==============================] - 27s 1ms/step\n",
            "[CV 2/2] END ............epochs=30, neurons=250;, score=0.455 total time=21.0min\n",
            "Epoch 1/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 6.9835\n",
            "Epoch 2/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 6.3562\n",
            "Epoch 3/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.3368\n",
            "Epoch 4/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.3284\n",
            "Epoch 5/30\n",
            "42501/42501 [==============================] - 66s 2ms/step - loss: 6.3209\n",
            "Epoch 6/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.3137\n",
            "Epoch 7/30\n",
            "42501/42501 [==============================] - 64s 2ms/step - loss: 6.3035\n",
            "Epoch 8/30\n",
            "42501/42501 [==============================] - 64s 2ms/step - loss: 6.2902\n",
            "Epoch 9/30\n",
            "42501/42501 [==============================] - 66s 2ms/step - loss: 6.1948\n",
            "Epoch 10/30\n",
            "42501/42501 [==============================] - 66s 2ms/step - loss: 6.1525\n",
            "Epoch 11/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.1393\n",
            "Epoch 12/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.1300\n",
            "Epoch 13/30\n",
            "42501/42501 [==============================] - 64s 2ms/step - loss: 6.1252\n",
            "Epoch 14/30\n",
            "42501/42501 [==============================] - 64s 2ms/step - loss: 6.1193\n",
            "Epoch 15/30\n",
            "42501/42501 [==============================] - 66s 2ms/step - loss: 5.9915\n",
            "Epoch 16/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.9569\n",
            "Epoch 17/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.9433\n",
            "Epoch 18/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.9380\n",
            "Epoch 19/30\n",
            "42501/42501 [==============================] - 68s 2ms/step - loss: 5.9322\n",
            "Epoch 20/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.8601\n",
            "Epoch 21/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.8153\n",
            "Epoch 22/30\n",
            "42501/42501 [==============================] - 66s 2ms/step - loss: 5.8005\n",
            "Epoch 23/30\n",
            "42501/42501 [==============================] - 68s 2ms/step - loss: 5.7923\n",
            "Epoch 24/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.7813\n",
            "Epoch 25/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.7709\n",
            "Epoch 26/30\n",
            "42501/42501 [==============================] - 66s 2ms/step - loss: 5.7667\n",
            "Epoch 27/30\n",
            "42501/42501 [==============================] - 67s 2ms/step - loss: 5.7606\n",
            "Epoch 28/30\n",
            "42501/42501 [==============================] - 70s 2ms/step - loss: 5.7586\n",
            "Epoch 29/30\n",
            "42501/42501 [==============================] - 72s 2ms/step - loss: 5.7532\n",
            "Epoch 30/30\n",
            "42501/42501 [==============================] - 73s 2ms/step - loss: 5.7413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# produce plot of model fit average r2 scores against number of estimators\n",
        "print(grid_result.best_score_)\n",
        "print(grid_result.best_params_)"
      ],
      "metadata": {
        "id": "zujsET2nAz0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4980a07-6d38-4b4d-9c49-f8632b615082"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5940418313992497\n",
            "{'epochs': 30, 'neurons': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ff_model(neurons=25, dropout_rate=0, activation='relu', init_mode='RandomNormal', learn_rate=0.001, optimizer='RMSprop'):\n",
        "    model=Sequential()\n",
        "    model.add(Dense(neurons,input_shape=(16,),kernel_initializer=init_mode, activation='linear'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation, kernel_regularizer='l2'))\n",
        "    #model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation))\n",
        "    #model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation))\n",
        "    #model.add(Dense(neurons,kernel_initializer=init_mode, activation=activation))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    #compile\n",
        "    model.compile(loss='mae', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Y6fBesFuj6ys"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test model with best params on hold out test dataset\n",
        "model = KerasRegressor(build_fn=create_ff_model, batch_size=10, epochs =30)\n",
        "model.fit(trainx_sc,trainy)"
      ],
      "metadata": {
        "id": "3TSVBHW2KY2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d51bb4be-48c2-4ddc-81ba-62db0365533d"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-114-3b3685e733cc>:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasRegressor(build_fn=create_ff_model, batch_size=10, epochs =30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.9447\n",
            "Epoch 2/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.3562\n",
            "Epoch 3/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.3085\n",
            "Epoch 4/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.2732\n",
            "Epoch 5/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.2524\n",
            "Epoch 6/30\n",
            "42501/42501 [==============================] - 65s 2ms/step - loss: 6.2443\n",
            "Epoch 7/30\n",
            "42501/42501 [==============================] - 63s 1ms/step - loss: 6.2379\n",
            "Epoch 8/30\n",
            "42501/42501 [==============================] - 63s 1ms/step - loss: 6.1157\n",
            "Epoch 9/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0740\n",
            "Epoch 10/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0593\n",
            "Epoch 11/30\n",
            "42501/42501 [==============================] - 63s 1ms/step - loss: 6.0543\n",
            "Epoch 12/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0493\n",
            "Epoch 13/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0456\n",
            "Epoch 14/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0426\n",
            "Epoch 15/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0410\n",
            "Epoch 16/30\n",
            "42501/42501 [==============================] - 63s 1ms/step - loss: 6.0390\n",
            "Epoch 17/30\n",
            "42501/42501 [==============================] - 63s 1ms/step - loss: 6.0365\n",
            "Epoch 18/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0349\n",
            "Epoch 19/30\n",
            "42501/42501 [==============================] - 61s 1ms/step - loss: 6.0329\n",
            "Epoch 20/30\n",
            "42501/42501 [==============================] - 61s 1ms/step - loss: 6.0301\n",
            "Epoch 21/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0283\n",
            "Epoch 22/30\n",
            "42501/42501 [==============================] - 61s 1ms/step - loss: 6.0265\n",
            "Epoch 23/30\n",
            "42501/42501 [==============================] - 60s 1ms/step - loss: 6.0269\n",
            "Epoch 24/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0262\n",
            "Epoch 25/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0243\n",
            "Epoch 26/30\n",
            "42501/42501 [==============================] - 61s 1ms/step - loss: 6.0231\n",
            "Epoch 27/30\n",
            "42501/42501 [==============================] - 63s 1ms/step - loss: 6.0224\n",
            "Epoch 28/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0214\n",
            "Epoch 29/30\n",
            "42501/42501 [==============================] - 61s 1ms/step - loss: 6.0218\n",
            "Epoch 30/30\n",
            "42501/42501 [==============================] - 62s 1ms/step - loss: 6.0195\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb40751a4a0>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testx = test_df.drop(columns=['VHI'])\n",
        "testy = test_df['VHI']\n",
        "testx_sc = scaler.transform(testx)"
      ],
      "metadata": {
        "id": "Te5KDKBHPvsW"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(testx_sc)"
      ],
      "metadata": {
        "id": "g5sAsRErPUdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2017a76-b5b8-4545-ee7a-a68f2f24f0e4"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15268/15268 [==============================] - 17s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(r2_score(testy,predictions))\n",
        "test_r2_score = r2_score(testy,predictions)"
      ],
      "metadata": {
        "id": "lxYFoox-P__x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a312db-7543-4b76-8461-b865ca025b54"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6036359923676962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get line plot of estimators vs R2 performance\n",
        "# save model outputs to use in plotting script\n",
        "print(len(predictions))\n",
        "print(predictions.shape)\n",
        "print(testx)"
      ],
      "metadata": {
        "id": "EgdWwUkvJFRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626e10b4-fdf7-4082-a39e-62d449816b3e"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "152680\n",
            "(152680,)\n",
            "           lon     lat      vhi-1    precip-1     rzsm-1  spei1-1m  spei2-1m  \\\n",
            "32     -53.375 -32.125  33.460501   31.077044  43.486305 -1.947763 -0.260165   \n",
            "33     -53.375 -32.125  53.849729  115.865265  20.373812  0.641211 -0.586411   \n",
            "34     -53.375 -32.125  62.237937   62.560310  22.396515 -1.185511 -0.421650   \n",
            "35     -53.375 -32.125  60.625543  104.338350  22.017498 -0.140137 -0.993312   \n",
            "36     -53.375 -32.125  64.313523   67.846820  23.099638 -0.963393 -0.731726   \n",
            "...        ...     ...        ...         ...        ...       ...       ...   \n",
            "577660 -47.625  -0.875  40.887218  273.721040  69.872240  0.256454 -0.125266   \n",
            "577661 -47.625  -0.875  40.887218  138.636630  49.382520 -0.555288 -0.168773   \n",
            "577662 -47.625  -0.875  50.892596  132.648000  52.529457  0.057421 -0.337412   \n",
            "577663 -47.625  -0.875  43.292766   18.556600  48.998238  0.275302  0.111811   \n",
            "577664 -47.625  -0.875  46.295286   36.166164  45.469177  1.248383  0.872078   \n",
            "\n",
            "        spei3-1m   spi01-1   spi02-1   spi03-1     pev-1   longrad-1  \\\n",
            "32      0.705498 -0.242212 -0.739179 -0.024334 -0.006122  351.888088   \n",
            "33     -1.465449 -0.480062  0.045311 -0.111329 -0.005500  373.469148   \n",
            "34     -1.065929 -0.285143 -0.545352 -0.201867 -0.004529  366.327420   \n",
            "35     -0.475748 -0.339369 -0.454374 -0.655996 -0.003037  347.575508   \n",
            "36     -1.305356 -0.112639 -0.374774 -0.510010 -0.001843  320.167262   \n",
            "...          ...       ...       ...       ...       ...         ...   \n",
            "577660  0.722608 -0.035469  0.724087  0.535668 -0.004465  411.453620   \n",
            "577661 -0.320905  0.397892  0.152872  0.678610 -0.005219  410.579416   \n",
            "577662 -0.146564  1.039475  0.730772  0.422252 -0.006154  411.501680   \n",
            "577663 -0.228921  1.288790  1.500064  1.160143 -0.006485  412.035546   \n",
            "577664  0.487677  1.427190  1.714678  1.835401 -0.006285  415.294670   \n",
            "\n",
            "             t2m-1  month  year  \n",
            "32      293.945534      1  2006  \n",
            "33      296.343864      2  2006  \n",
            "34      295.021920      3  2006  \n",
            "35      291.600778      4  2006  \n",
            "36      286.381988      5  2006  \n",
            "...            ...    ...   ...  \n",
            "577660  299.832328      7  2019  \n",
            "577661  300.216304      8  2019  \n",
            "577662  300.688904      9  2019  \n",
            "577663  300.875056     10  2019  \n",
            "577664  300.780948     11  2019  \n",
            "\n",
            "[152680 rows x 16 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bring lat, lon, month and year together\n",
        "results_df = pd.DataFrame({'lat':testx['lat'],'lon':testx['lon'],'month':testx['month'],'year':testx['year'],'predictions':predictions,'testy':testy})"
      ],
      "metadata": {
        "id": "vcrJysSbClHM"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('/content/drive/MyDrive/model_outs/ANN_out_df_optimized.csv',index=False)"
      ],
      "metadata": {
        "id": "Tj-PI3C2EL39"
      },
      "execution_count": 120,
      "outputs": []
    }
  ]
}